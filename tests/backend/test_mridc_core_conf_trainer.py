# coding=utf-8
# Automatically generated by Pynguin.
import pytest

import mridc.core.conf.trainer as module_0


def test_case_0():
    var_0 = None
    int_0 = -2768
    int_1 = 49
    str_0 = "fq5r?IhM3Tt"
    bytes_0 = None
    bool_0 = False
    float_0 = -1760.55
    trainer_config_0 = module_0.TrainerConfig(
        var_0, int_0, int_0, int_1, str_0, int_0, str_0, bytes_0, bool_0, str_0, bytes_0, float_0, bytes_0
    )
    assert trainer_config_0.logger is None
    assert trainer_config_0.callbacks == -2768
    assert trainer_config_0.default_root_dir == -2768
    assert trainer_config_0.gradient_clip_val == 49
    assert trainer_config_0.num_nodes == "fq5r?IhM3Tt"
    assert trainer_config_0.gpus == -2768
    assert trainer_config_0.auto_select_gpus == "fq5r?IhM3Tt"
    assert trainer_config_0.tpu_cores is None
    assert trainer_config_0.enable_progress_bar is False
    assert trainer_config_0.overfit_batches == "fq5r?IhM3Tt"
    assert trainer_config_0.track_grad_norm is None
    assert trainer_config_0.check_val_every_n_epoch == pytest.approx(-1760.55, abs=0.01, rel=0.01)
    assert trainer_config_0.fast_dev_run is None
    assert trainer_config_0.accumulate_grad_batches == 1
    assert trainer_config_0.max_epochs == 1000
    assert trainer_config_0.min_epochs == 1
    assert trainer_config_0.max_steps == -1
    assert trainer_config_0.min_steps is None
    assert trainer_config_0.limit_train_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_0.limit_val_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_0.limit_test_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_0.val_check_interval == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_0.log_every_n_steps == 50
    assert trainer_config_0.accelerator is None
    assert trainer_config_0.sync_batchnorm is False
    assert trainer_config_0.precision == 32
    assert trainer_config_0.num_sanity_val_steps == 2
    assert trainer_config_0.resume_from_checkpoint is None
    assert trainer_config_0.profiler is None
    assert trainer_config_0.benchmark is False
    assert trainer_config_0.deterministic is False
    assert trainer_config_0.auto_lr_find is False
    assert trainer_config_0.replace_sampler_ddp is True
    assert trainer_config_0.detect_anomaly is False
    assert trainer_config_0.auto_scale_batch_size is False
    assert trainer_config_0.amp_backend == "native"
    assert trainer_config_0.amp_level is None
    assert trainer_config_0.plugins is None
    assert trainer_config_0.move_metrics_to_cpu is False
    assert trainer_config_0.multiple_trainloader_mode == "max_size_cycle"
    assert trainer_config_0.limit_predict_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_0.gradient_clip_algorithm == "norm"
    assert trainer_config_0.max_time is None
    assert trainer_config_0.reload_dataloaders_every_n_epochs == 0
    assert trainer_config_0.ipus is None
    assert trainer_config_0.devices is None
    assert trainer_config_0.strategy is None
    assert trainer_config_0.enable_checkpointing is False
    assert trainer_config_0.enable_model_summary is True
    assert module_0.TrainerConfig.logger is True
    assert module_0.TrainerConfig.callbacks is None
    assert module_0.TrainerConfig.default_root_dir is None
    assert module_0.TrainerConfig.gradient_clip_val == 0
    assert module_0.TrainerConfig.num_nodes == 1
    assert module_0.TrainerConfig.gpus is None
    assert module_0.TrainerConfig.auto_select_gpus is False
    assert module_0.TrainerConfig.tpu_cores is None
    assert module_0.TrainerConfig.enable_progress_bar is True
    assert module_0.TrainerConfig.overfit_batches == pytest.approx(0.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.track_grad_norm == -1
    assert module_0.TrainerConfig.check_val_every_n_epoch == 1
    assert module_0.TrainerConfig.fast_dev_run is False
    assert module_0.TrainerConfig.accumulate_grad_batches == 1
    assert module_0.TrainerConfig.max_epochs == 1000
    assert module_0.TrainerConfig.min_epochs == 1
    assert module_0.TrainerConfig.max_steps == -1
    assert module_0.TrainerConfig.min_steps is None
    assert module_0.TrainerConfig.limit_train_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.limit_val_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.limit_test_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.val_check_interval == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.log_every_n_steps == 50
    assert module_0.TrainerConfig.accelerator is None
    assert module_0.TrainerConfig.sync_batchnorm is False
    assert module_0.TrainerConfig.precision == 32
    assert module_0.TrainerConfig.num_sanity_val_steps == 2
    assert module_0.TrainerConfig.resume_from_checkpoint is None
    assert module_0.TrainerConfig.profiler is None
    assert module_0.TrainerConfig.benchmark is False
    assert module_0.TrainerConfig.deterministic is False
    assert module_0.TrainerConfig.auto_lr_find is False
    assert module_0.TrainerConfig.replace_sampler_ddp is True
    assert module_0.TrainerConfig.detect_anomaly is False
    assert module_0.TrainerConfig.auto_scale_batch_size is False
    assert module_0.TrainerConfig.amp_backend == "native"
    assert module_0.TrainerConfig.amp_level is None
    assert module_0.TrainerConfig.plugins is None
    assert module_0.TrainerConfig.move_metrics_to_cpu is False
    assert module_0.TrainerConfig.multiple_trainloader_mode == "max_size_cycle"
    assert module_0.TrainerConfig.limit_predict_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.gradient_clip_algorithm == "norm"
    assert module_0.TrainerConfig.max_time is None
    assert module_0.TrainerConfig.reload_dataloaders_every_n_epochs == 0
    assert module_0.TrainerConfig.ipus is None
    assert module_0.TrainerConfig.devices is None
    assert module_0.TrainerConfig.strategy is None
    assert module_0.TrainerConfig.enable_checkpointing is False
    assert module_0.TrainerConfig.enable_model_summary is True
    str_1 = "Target"
    int_2 = 1433
    list_0 = [str_1, int_2, str_1]
    bool_1 = True
    bool_2 = False
    int_3 = 96
    str_2 = ', available options are ["mean", "sum", "none"].'
    dict_0 = {bool_1: str_2, bool_2: bool_1}
    bytes_1 = b'\x97\x01=\x9a}"\xc0"2'
    tuple_0 = dict_0, bytes_1, str_1, dict_0
    bool_3 = True
    var_1 = None
    bool_4 = True
    bool_5 = True
    trainer_config_1 = module_0.TrainerConfig(
        bool_2, tuple_0, tuple_0, bool_2, int_3, var_1, bool_2, list_0, str_2, tuple_0, bool_4, int_3, bool_5
    )
    assert trainer_config_1.logger is False
    assert trainer_config_1.callbacks == (
        {(True): ', available options are ["mean", "sum", "none"].', (False): True},
        b'\x97\x01=\x9a}"\xc0"2',
        "Target",
        {(True): ', available options are ["mean", "sum", "none"].', (False): True},
    )
    assert trainer_config_1.default_root_dir == (
        {(True): ', available options are ["mean", "sum", "none"].', (False): True},
        b'\x97\x01=\x9a}"\xc0"2',
        "Target",
        {(True): ', available options are ["mean", "sum", "none"].', (False): True},
    )
    assert trainer_config_1.gradient_clip_val is False
    assert trainer_config_1.num_nodes == 96
    assert trainer_config_1.gpus is None
    assert trainer_config_1.auto_select_gpus is False
    assert trainer_config_1.tpu_cores == ["Target", 1433, "Target"]
    assert trainer_config_1.enable_progress_bar == ', available options are ["mean", "sum", "none"].'
    assert trainer_config_1.overfit_batches == (
        {(True): ', available options are ["mean", "sum", "none"].', (False): True},
        b'\x97\x01=\x9a}"\xc0"2',
        "Target",
        {(True): ', available options are ["mean", "sum", "none"].', (False): True},
    )
    assert trainer_config_1.track_grad_norm is True
    assert trainer_config_1.check_val_every_n_epoch == 96
    assert trainer_config_1.fast_dev_run is True
    assert trainer_config_1.accumulate_grad_batches == 1
    assert trainer_config_1.max_epochs == 1000
    assert trainer_config_1.min_epochs == 1
    assert trainer_config_1.max_steps == -1
    assert trainer_config_1.min_steps is None
    assert trainer_config_1.limit_train_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_1.limit_val_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_1.limit_test_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_1.val_check_interval == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_1.log_every_n_steps == 50
    assert trainer_config_1.accelerator is None
    assert trainer_config_1.sync_batchnorm is False
    assert trainer_config_1.precision == 32
    assert trainer_config_1.num_sanity_val_steps == 2
    assert trainer_config_1.resume_from_checkpoint is None
    assert trainer_config_1.profiler is None
    assert trainer_config_1.benchmark is False
    assert trainer_config_1.deterministic is False
    assert trainer_config_1.auto_lr_find is False
    assert trainer_config_1.replace_sampler_ddp is True
    assert trainer_config_1.detect_anomaly is False
    assert trainer_config_1.auto_scale_batch_size is False
    assert trainer_config_1.amp_backend == "native"
    assert trainer_config_1.amp_level is None
    assert trainer_config_1.plugins is None
    assert trainer_config_1.move_metrics_to_cpu is False
    assert trainer_config_1.multiple_trainloader_mode == "max_size_cycle"
    assert trainer_config_1.limit_predict_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_1.gradient_clip_algorithm == "norm"
    assert trainer_config_1.max_time is None
    assert trainer_config_1.reload_dataloaders_every_n_epochs == 0
    assert trainer_config_1.ipus is None
    assert trainer_config_1.devices is None
    assert trainer_config_1.strategy is None
    assert trainer_config_1.enable_checkpointing is False
    assert trainer_config_1.enable_model_summary is True
    bool_6 = False
    none_type_0 = None
    int_4 = 2665
    str_3 = "g=)"
    trainer_config_2 = module_0.TrainerConfig(str_2, int_2, int_4, bool_2, bool_3, int_3, str_3, bool_1)
    assert trainer_config_2.logger == ', available options are ["mean", "sum", "none"].'
    assert trainer_config_2.callbacks == 1433
    assert trainer_config_2.default_root_dir == 2665
    assert trainer_config_2.gradient_clip_val is False
    assert trainer_config_2.num_nodes is True
    assert trainer_config_2.gpus == 96
    assert trainer_config_2.auto_select_gpus == "g=)"
    assert trainer_config_2.tpu_cores is True
    assert trainer_config_2.enable_progress_bar is True
    assert trainer_config_2.overfit_batches == pytest.approx(0.0, abs=0.01, rel=0.01)
    assert trainer_config_2.track_grad_norm == -1
    assert trainer_config_2.check_val_every_n_epoch == 1
    assert trainer_config_2.fast_dev_run is False
    assert trainer_config_2.accumulate_grad_batches == 1
    assert trainer_config_2.max_epochs == 1000
    assert trainer_config_2.min_epochs == 1
    assert trainer_config_2.max_steps == -1
    assert trainer_config_2.min_steps is None
    assert trainer_config_2.limit_train_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_2.limit_val_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_2.limit_test_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_2.val_check_interval == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_2.log_every_n_steps == 50
    assert trainer_config_2.accelerator is None
    assert trainer_config_2.sync_batchnorm is False
    assert trainer_config_2.precision == 32
    assert trainer_config_2.num_sanity_val_steps == 2
    assert trainer_config_2.resume_from_checkpoint is None
    assert trainer_config_2.profiler is None
    assert trainer_config_2.benchmark is False
    assert trainer_config_2.deterministic is False
    assert trainer_config_2.auto_lr_find is False
    assert trainer_config_2.replace_sampler_ddp is True
    assert trainer_config_2.detect_anomaly is False
    assert trainer_config_2.auto_scale_batch_size is False
    assert trainer_config_2.amp_backend == "native"
    assert trainer_config_2.amp_level is None
    assert trainer_config_2.plugins is None
    assert trainer_config_2.move_metrics_to_cpu is False
    assert trainer_config_2.multiple_trainloader_mode == "max_size_cycle"
    assert trainer_config_2.limit_predict_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_2.gradient_clip_algorithm == "norm"
    assert trainer_config_2.max_time is None
    assert trainer_config_2.reload_dataloaders_every_n_epochs == 0
    assert trainer_config_2.ipus is None
    assert trainer_config_2.devices is None
    assert trainer_config_2.strategy is None
    assert trainer_config_2.enable_checkpointing is False
    assert trainer_config_2.enable_model_summary is True
    trainer_config_3 = module_0.TrainerConfig(
        str_1, list_0, bool_1, list_0, bool_1, bool_2, int_2, int_3, str_1, tuple_0, bool_3, bool_6, none_type_0, int_3
    )
    assert trainer_config_3.logger == "Target"
    assert trainer_config_3.callbacks == ["Target", 1433, "Target"]
    assert trainer_config_3.default_root_dir is True
    assert trainer_config_3.gradient_clip_val == ["Target", 1433, "Target"]
    assert trainer_config_3.num_nodes is True
    assert trainer_config_3.gpus is False
    assert trainer_config_3.auto_select_gpus == 1433
    assert trainer_config_3.tpu_cores == 96
    assert trainer_config_3.enable_progress_bar == "Target"
    assert trainer_config_3.overfit_batches == (
        {(True): ', available options are ["mean", "sum", "none"].', (False): True},
        b'\x97\x01=\x9a}"\xc0"2',
        "Target",
        {(True): ', available options are ["mean", "sum", "none"].', (False): True},
    )
    assert trainer_config_3.track_grad_norm is True
    assert trainer_config_3.check_val_every_n_epoch is False
    assert trainer_config_3.fast_dev_run is None
    assert trainer_config_3.accumulate_grad_batches == 96
    assert trainer_config_3.max_epochs == 1000
    assert trainer_config_3.min_epochs == 1
    assert trainer_config_3.max_steps == -1
    assert trainer_config_3.min_steps is None
    assert trainer_config_3.limit_train_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_3.limit_val_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_3.limit_test_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_3.val_check_interval == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_3.log_every_n_steps == 50
    assert trainer_config_3.accelerator is None
    assert trainer_config_3.sync_batchnorm is False
    assert trainer_config_3.precision == 32
    assert trainer_config_3.num_sanity_val_steps == 2
    assert trainer_config_3.resume_from_checkpoint is None
    assert trainer_config_3.profiler is None
    assert trainer_config_3.benchmark is False
    assert trainer_config_3.deterministic is False
    assert trainer_config_3.auto_lr_find is False
    assert trainer_config_3.replace_sampler_ddp is True
    assert trainer_config_3.detect_anomaly is False
    assert trainer_config_3.auto_scale_batch_size is False
    assert trainer_config_3.amp_backend == "native"
    assert trainer_config_3.amp_level is None
    assert trainer_config_3.plugins is None
    assert trainer_config_3.move_metrics_to_cpu is False
    assert trainer_config_3.multiple_trainloader_mode == "max_size_cycle"
    assert trainer_config_3.limit_predict_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_3.gradient_clip_algorithm == "norm"
    assert trainer_config_3.max_time is None
    assert trainer_config_3.reload_dataloaders_every_n_epochs == 0
    assert trainer_config_3.ipus is None
    assert trainer_config_3.devices is None
    assert trainer_config_3.strategy is None
    assert trainer_config_3.enable_checkpointing is False
    assert trainer_config_3.enable_model_summary is True


def test_case_1():
    float_0 = -2850.6
    any_0 = None
    bool_0 = None
    bool_1 = False
    int_0 = 3800
    bool_2 = True
    str_0 = "'3oS-\n"
    trainer_config_0 = module_0.TrainerConfig(
        float_0, any_0, bool_0, any_0, bool_1, int_0, bool_1, any_0, bool_2, str_0, bool_2, int_0
    )
    assert trainer_config_0.logger == pytest.approx(-2850.6, abs=0.01, rel=0.01)
    assert trainer_config_0.callbacks is None
    assert trainer_config_0.default_root_dir is None
    assert trainer_config_0.gradient_clip_val is None
    assert trainer_config_0.num_nodes is False
    assert trainer_config_0.gpus == 3800
    assert trainer_config_0.auto_select_gpus is False
    assert trainer_config_0.tpu_cores is None
    assert trainer_config_0.enable_progress_bar is True
    assert trainer_config_0.overfit_batches == "'3oS-\n"
    assert trainer_config_0.track_grad_norm is True
    assert trainer_config_0.check_val_every_n_epoch == 3800
    assert trainer_config_0.fast_dev_run is False
    assert trainer_config_0.accumulate_grad_batches == 1
    assert trainer_config_0.max_epochs == 1000
    assert trainer_config_0.min_epochs == 1
    assert trainer_config_0.max_steps == -1
    assert trainer_config_0.min_steps is None
    assert trainer_config_0.limit_train_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_0.limit_val_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_0.limit_test_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_0.val_check_interval == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_0.log_every_n_steps == 50
    assert trainer_config_0.accelerator is None
    assert trainer_config_0.sync_batchnorm is False
    assert trainer_config_0.precision == 32
    assert trainer_config_0.num_sanity_val_steps == 2
    assert trainer_config_0.resume_from_checkpoint is None
    assert trainer_config_0.profiler is None
    assert trainer_config_0.benchmark is False
    assert trainer_config_0.deterministic is False
    assert trainer_config_0.auto_lr_find is False
    assert trainer_config_0.replace_sampler_ddp is True
    assert trainer_config_0.detect_anomaly is False
    assert trainer_config_0.auto_scale_batch_size is False
    assert trainer_config_0.amp_backend == "native"
    assert trainer_config_0.amp_level is None
    assert trainer_config_0.plugins is None
    assert trainer_config_0.move_metrics_to_cpu is False
    assert trainer_config_0.multiple_trainloader_mode == "max_size_cycle"
    assert trainer_config_0.limit_predict_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_0.gradient_clip_algorithm == "norm"
    assert trainer_config_0.max_time is None
    assert trainer_config_0.reload_dataloaders_every_n_epochs == 0
    assert trainer_config_0.ipus is None
    assert trainer_config_0.devices is None
    assert trainer_config_0.strategy is None
    assert trainer_config_0.enable_checkpointing is False
    assert trainer_config_0.enable_model_summary is True
    assert module_0.TrainerConfig.logger is True
    assert module_0.TrainerConfig.callbacks is None
    assert module_0.TrainerConfig.default_root_dir is None
    assert module_0.TrainerConfig.gradient_clip_val == 0
    assert module_0.TrainerConfig.num_nodes == 1
    assert module_0.TrainerConfig.gpus is None
    assert module_0.TrainerConfig.auto_select_gpus is False
    assert module_0.TrainerConfig.tpu_cores is None
    assert module_0.TrainerConfig.enable_progress_bar is True
    assert module_0.TrainerConfig.overfit_batches == pytest.approx(0.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.track_grad_norm == -1
    assert module_0.TrainerConfig.check_val_every_n_epoch == 1
    assert module_0.TrainerConfig.fast_dev_run is False
    assert module_0.TrainerConfig.accumulate_grad_batches == 1
    assert module_0.TrainerConfig.max_epochs == 1000
    assert module_0.TrainerConfig.min_epochs == 1
    assert module_0.TrainerConfig.max_steps == -1
    assert module_0.TrainerConfig.min_steps is None
    assert module_0.TrainerConfig.limit_train_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.limit_val_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.limit_test_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.val_check_interval == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.log_every_n_steps == 50
    assert module_0.TrainerConfig.accelerator is None
    assert module_0.TrainerConfig.sync_batchnorm is False
    assert module_0.TrainerConfig.precision == 32
    assert module_0.TrainerConfig.num_sanity_val_steps == 2
    assert module_0.TrainerConfig.resume_from_checkpoint is None
    assert module_0.TrainerConfig.profiler is None
    assert module_0.TrainerConfig.benchmark is False
    assert module_0.TrainerConfig.deterministic is False
    assert module_0.TrainerConfig.auto_lr_find is False
    assert module_0.TrainerConfig.replace_sampler_ddp is True
    assert module_0.TrainerConfig.detect_anomaly is False
    assert module_0.TrainerConfig.auto_scale_batch_size is False
    assert module_0.TrainerConfig.amp_backend == "native"
    assert module_0.TrainerConfig.amp_level is None
    assert module_0.TrainerConfig.plugins is None
    assert module_0.TrainerConfig.move_metrics_to_cpu is False
    assert module_0.TrainerConfig.multiple_trainloader_mode == "max_size_cycle"
    assert module_0.TrainerConfig.limit_predict_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.gradient_clip_algorithm == "norm"
    assert module_0.TrainerConfig.max_time is None
    assert module_0.TrainerConfig.reload_dataloaders_every_n_epochs == 0
    assert module_0.TrainerConfig.ipus is None
    assert module_0.TrainerConfig.devices is None
    assert module_0.TrainerConfig.strategy is None
    assert module_0.TrainerConfig.enable_checkpointing is False
    assert module_0.TrainerConfig.enable_model_summary is True


def test_case_2():
    var_0 = None
    str_0 = "./+UX?\x0b%u?Zb6?"
    int_0 = -5283
    bool_0 = None
    bool_1 = False
    bool_2 = True
    float_0 = -2434.11453
    trainer_config_0 = module_0.TrainerConfig(
        var_0, str_0, int_0, bool_0, int_0, str_0, bool_1, str_0, bool_2, float_0, bool_0
    )
    assert trainer_config_0.logger is None
    assert trainer_config_0.callbacks == "./+UX?\x0b%u?Zb6?"
    assert trainer_config_0.default_root_dir == -5283
    assert trainer_config_0.gradient_clip_val is None
    assert trainer_config_0.num_nodes == -5283
    assert trainer_config_0.gpus == "./+UX?\x0b%u?Zb6?"
    assert trainer_config_0.auto_select_gpus is False
    assert trainer_config_0.tpu_cores == "./+UX?\x0b%u?Zb6?"
    assert trainer_config_0.enable_progress_bar is True
    assert trainer_config_0.overfit_batches == pytest.approx(-2434.11453, abs=0.01, rel=0.01)
    assert trainer_config_0.track_grad_norm is None
    assert trainer_config_0.check_val_every_n_epoch == 1
    assert trainer_config_0.fast_dev_run is False
    assert trainer_config_0.accumulate_grad_batches == 1
    assert trainer_config_0.max_epochs == 1000
    assert trainer_config_0.min_epochs == 1
    assert trainer_config_0.max_steps == -1
    assert trainer_config_0.min_steps is None
    assert trainer_config_0.limit_train_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_0.limit_val_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_0.limit_test_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_0.val_check_interval == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_0.log_every_n_steps == 50
    assert trainer_config_0.accelerator is None
    assert trainer_config_0.sync_batchnorm is False
    assert trainer_config_0.precision == 32
    assert trainer_config_0.num_sanity_val_steps == 2
    assert trainer_config_0.resume_from_checkpoint is None
    assert trainer_config_0.profiler is None
    assert trainer_config_0.benchmark is False
    assert trainer_config_0.deterministic is False
    assert trainer_config_0.auto_lr_find is False
    assert trainer_config_0.replace_sampler_ddp is True
    assert trainer_config_0.detect_anomaly is False
    assert trainer_config_0.auto_scale_batch_size is False
    assert trainer_config_0.amp_backend == "native"
    assert trainer_config_0.amp_level is None
    assert trainer_config_0.plugins is None
    assert trainer_config_0.move_metrics_to_cpu is False
    assert trainer_config_0.multiple_trainloader_mode == "max_size_cycle"
    assert trainer_config_0.limit_predict_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_0.gradient_clip_algorithm == "norm"
    assert trainer_config_0.max_time is None
    assert trainer_config_0.reload_dataloaders_every_n_epochs == 0
    assert trainer_config_0.ipus is None
    assert trainer_config_0.devices is None
    assert trainer_config_0.strategy is None
    assert trainer_config_0.enable_checkpointing is False
    assert trainer_config_0.enable_model_summary is True
    assert module_0.TrainerConfig.logger is True
    assert module_0.TrainerConfig.callbacks is None
    assert module_0.TrainerConfig.default_root_dir is None
    assert module_0.TrainerConfig.gradient_clip_val == 0
    assert module_0.TrainerConfig.num_nodes == 1
    assert module_0.TrainerConfig.gpus is None
    assert module_0.TrainerConfig.auto_select_gpus is False
    assert module_0.TrainerConfig.tpu_cores is None
    assert module_0.TrainerConfig.enable_progress_bar is True
    assert module_0.TrainerConfig.overfit_batches == pytest.approx(0.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.track_grad_norm == -1
    assert module_0.TrainerConfig.check_val_every_n_epoch == 1
    assert module_0.TrainerConfig.fast_dev_run is False
    assert module_0.TrainerConfig.accumulate_grad_batches == 1
    assert module_0.TrainerConfig.max_epochs == 1000
    assert module_0.TrainerConfig.min_epochs == 1
    assert module_0.TrainerConfig.max_steps == -1
    assert module_0.TrainerConfig.min_steps is None
    assert module_0.TrainerConfig.limit_train_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.limit_val_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.limit_test_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.val_check_interval == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.log_every_n_steps == 50
    assert module_0.TrainerConfig.accelerator is None
    assert module_0.TrainerConfig.sync_batchnorm is False
    assert module_0.TrainerConfig.precision == 32
    assert module_0.TrainerConfig.num_sanity_val_steps == 2
    assert module_0.TrainerConfig.resume_from_checkpoint is None
    assert module_0.TrainerConfig.profiler is None
    assert module_0.TrainerConfig.benchmark is False
    assert module_0.TrainerConfig.deterministic is False
    assert module_0.TrainerConfig.auto_lr_find is False
    assert module_0.TrainerConfig.replace_sampler_ddp is True
    assert module_0.TrainerConfig.detect_anomaly is False
    assert module_0.TrainerConfig.auto_scale_batch_size is False
    assert module_0.TrainerConfig.amp_backend == "native"
    assert module_0.TrainerConfig.amp_level is None
    assert module_0.TrainerConfig.plugins is None
    assert module_0.TrainerConfig.move_metrics_to_cpu is False
    assert module_0.TrainerConfig.multiple_trainloader_mode == "max_size_cycle"
    assert module_0.TrainerConfig.limit_predict_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.gradient_clip_algorithm == "norm"
    assert module_0.TrainerConfig.max_time is None
    assert module_0.TrainerConfig.reload_dataloaders_every_n_epochs == 0
    assert module_0.TrainerConfig.ipus is None
    assert module_0.TrainerConfig.devices is None
    assert module_0.TrainerConfig.strategy is None
    assert module_0.TrainerConfig.enable_checkpointing is False
    assert module_0.TrainerConfig.enable_model_summary is True
    bool_3 = False
    bytes_0 = b"\xde\xe2z\x0b\x00z\x82\xec\xd1\xbf"
    tuple_0 = bool_3, bytes_0
    int_1 = 2083
    str_1 = "P@w %3NHryNf]*O"
    bool_4 = False
    bool_5 = None
    trainer_config_1 = None
    int_2 = 39
    tuple_1 = ()
    int_3 = 646
    bool_6 = False
    list_0 = [trainer_config_0]
    trainer_config_2 = module_0.TrainerConfig(
        trainer_config_1,
        int_2,
        tuple_1,
        int_0,
        str_0,
        bool_2,
        str_1,
        int_3,
        bool_3,
        bool_6,
        bool_2,
        str_0,
        bool_3,
        int_3,
        list_0,
    )
    assert trainer_config_2.logger is None
    assert trainer_config_2.callbacks == 39
    assert trainer_config_2.default_root_dir == ()
    assert trainer_config_2.gradient_clip_val == -5283
    assert trainer_config_2.num_nodes == "./+UX?\x0b%u?Zb6?"
    assert trainer_config_2.gpus is True
    assert trainer_config_2.auto_select_gpus == "P@w %3NHryNf]*O"
    assert trainer_config_2.tpu_cores == 646
    assert trainer_config_2.enable_progress_bar is False
    assert trainer_config_2.overfit_batches is False
    assert trainer_config_2.track_grad_norm is True
    assert trainer_config_2.check_val_every_n_epoch == "./+UX?\x0b%u?Zb6?"
    assert trainer_config_2.fast_dev_run is False
    assert trainer_config_2.accumulate_grad_batches == 646
    assert len(trainer_config_2.max_epochs) == 1
    assert trainer_config_2.min_epochs == 1
    assert trainer_config_2.max_steps == -1
    assert trainer_config_2.min_steps is None
    assert trainer_config_2.limit_train_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_2.limit_val_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_2.limit_test_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_2.val_check_interval == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_2.log_every_n_steps == 50
    assert trainer_config_2.accelerator is None
    assert trainer_config_2.sync_batchnorm is False
    assert trainer_config_2.precision == 32
    assert trainer_config_2.num_sanity_val_steps == 2
    assert trainer_config_2.resume_from_checkpoint is None
    assert trainer_config_2.profiler is None
    assert trainer_config_2.benchmark is False
    assert trainer_config_2.deterministic is False
    assert trainer_config_2.auto_lr_find is False
    assert trainer_config_2.replace_sampler_ddp is True
    assert trainer_config_2.detect_anomaly is False
    assert trainer_config_2.auto_scale_batch_size is False
    assert trainer_config_2.amp_backend == "native"
    assert trainer_config_2.amp_level is None
    assert trainer_config_2.plugins is None
    assert trainer_config_2.move_metrics_to_cpu is False
    assert trainer_config_2.multiple_trainloader_mode == "max_size_cycle"
    assert trainer_config_2.limit_predict_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_2.gradient_clip_algorithm == "norm"
    assert trainer_config_2.max_time is None
    assert trainer_config_2.reload_dataloaders_every_n_epochs == 0
    assert trainer_config_2.ipus is None
    assert trainer_config_2.devices is None
    assert trainer_config_2.strategy is None
    assert trainer_config_2.enable_checkpointing is False
    assert trainer_config_2.enable_model_summary is True
    bool_7 = True
    float_1 = -1771.0
    int_4 = 31
    trainer_config_3 = module_0.TrainerConfig(
        var_0,
        tuple_0,
        int_1,
        int_1,
        int_1,
        str_1,
        bool_3,
        bool_4,
        bool_4,
        bool_5,
        bool_7,
        str_1,
        str_1,
        float_1,
        int_4,
    )
    assert trainer_config_3.logger is None
    assert trainer_config_3.callbacks == (False, b"\xde\xe2z\x0b\x00z\x82\xec\xd1\xbf")
    assert trainer_config_3.default_root_dir == 2083
    assert trainer_config_3.gradient_clip_val == 2083
    assert trainer_config_3.num_nodes == 2083
    assert trainer_config_3.gpus == "P@w %3NHryNf]*O"
    assert trainer_config_3.auto_select_gpus is False
    assert trainer_config_3.tpu_cores is False
    assert trainer_config_3.enable_progress_bar is False
    assert trainer_config_3.overfit_batches is None
    assert trainer_config_3.track_grad_norm is True
    assert trainer_config_3.check_val_every_n_epoch == "P@w %3NHryNf]*O"
    assert trainer_config_3.fast_dev_run == "P@w %3NHryNf]*O"
    assert trainer_config_3.accumulate_grad_batches == pytest.approx(-1771.0, abs=0.01, rel=0.01)
    assert trainer_config_3.max_epochs == 31
    assert trainer_config_3.min_epochs == 1
    assert trainer_config_3.max_steps == -1
    assert trainer_config_3.min_steps is None
    assert trainer_config_3.limit_train_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_3.limit_val_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_3.limit_test_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_3.val_check_interval == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_3.log_every_n_steps == 50
    assert trainer_config_3.accelerator is None
    assert trainer_config_3.sync_batchnorm is False
    assert trainer_config_3.precision == 32
    assert trainer_config_3.num_sanity_val_steps == 2
    assert trainer_config_3.resume_from_checkpoint is None
    assert trainer_config_3.profiler is None
    assert trainer_config_3.benchmark is False
    assert trainer_config_3.deterministic is False
    assert trainer_config_3.auto_lr_find is False
    assert trainer_config_3.replace_sampler_ddp is True
    assert trainer_config_3.detect_anomaly is False
    assert trainer_config_3.auto_scale_batch_size is False
    assert trainer_config_3.amp_backend == "native"
    assert trainer_config_3.amp_level is None
    assert trainer_config_3.plugins is None
    assert trainer_config_3.move_metrics_to_cpu is False
    assert trainer_config_3.multiple_trainloader_mode == "max_size_cycle"
    assert trainer_config_3.limit_predict_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_3.gradient_clip_algorithm == "norm"
    assert trainer_config_3.max_time is None
    assert trainer_config_3.reload_dataloaders_every_n_epochs == 0
    assert trainer_config_3.ipus is None
    assert trainer_config_3.devices is None
    assert trainer_config_3.strategy is None
    assert trainer_config_3.enable_checkpointing is False
    assert trainer_config_3.enable_model_summary is True


def test_case_3():
    float_0 = None
    str_0 = "d%D"
    bool_0 = None
    int_0 = 275
    bool_1 = False
    set_0 = {str_0, float_0, bool_0, float_0}
    trainer_config_0 = module_0.TrainerConfig(
        float_0,
        str_0,
        str_0,
        bool_0,
        int_0,
        int_0,
        int_0,
        float_0,
        float_0,
        str_0,
        str_0,
        str_0,
        bool_1,
        str_0,
        set_0,
        int_0,
        bool_0,
        bool_1,
    )
    assert trainer_config_0.logger is None
    assert trainer_config_0.callbacks == "d%D"
    assert trainer_config_0.default_root_dir == "d%D"
    assert trainer_config_0.gradient_clip_val is None
    assert trainer_config_0.num_nodes == 275
    assert trainer_config_0.gpus == 275
    assert trainer_config_0.auto_select_gpus == 275
    assert trainer_config_0.tpu_cores is None
    assert trainer_config_0.enable_progress_bar is None
    assert trainer_config_0.overfit_batches == "d%D"
    assert trainer_config_0.track_grad_norm == "d%D"
    assert trainer_config_0.check_val_every_n_epoch == "d%D"
    assert trainer_config_0.fast_dev_run is False
    assert trainer_config_0.accumulate_grad_batches == "d%D"
    assert trainer_config_0.max_epochs == {None, "d%D"}
    assert trainer_config_0.min_epochs == 275
    assert trainer_config_0.max_steps is None
    assert trainer_config_0.min_steps is False
    assert trainer_config_0.limit_train_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_0.limit_val_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_0.limit_test_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_0.val_check_interval == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_0.log_every_n_steps == 50
    assert trainer_config_0.accelerator is None
    assert trainer_config_0.sync_batchnorm is False
    assert trainer_config_0.precision == 32
    assert trainer_config_0.num_sanity_val_steps == 2
    assert trainer_config_0.resume_from_checkpoint is None
    assert trainer_config_0.profiler is None
    assert trainer_config_0.benchmark is False
    assert trainer_config_0.deterministic is False
    assert trainer_config_0.auto_lr_find is False
    assert trainer_config_0.replace_sampler_ddp is True
    assert trainer_config_0.detect_anomaly is False
    assert trainer_config_0.auto_scale_batch_size is False
    assert trainer_config_0.amp_backend == "native"
    assert trainer_config_0.amp_level is None
    assert trainer_config_0.plugins is None
    assert trainer_config_0.move_metrics_to_cpu is False
    assert trainer_config_0.multiple_trainloader_mode == "max_size_cycle"
    assert trainer_config_0.limit_predict_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_0.gradient_clip_algorithm == "norm"
    assert trainer_config_0.max_time is None
    assert trainer_config_0.reload_dataloaders_every_n_epochs == 0
    assert trainer_config_0.ipus is None
    assert trainer_config_0.devices is None
    assert trainer_config_0.strategy is None
    assert trainer_config_0.enable_checkpointing is False
    assert trainer_config_0.enable_model_summary is True
    assert module_0.TrainerConfig.logger is True
    assert module_0.TrainerConfig.callbacks is None
    assert module_0.TrainerConfig.default_root_dir is None
    assert module_0.TrainerConfig.gradient_clip_val == 0
    assert module_0.TrainerConfig.num_nodes == 1
    assert module_0.TrainerConfig.gpus is None
    assert module_0.TrainerConfig.auto_select_gpus is False
    assert module_0.TrainerConfig.tpu_cores is None
    assert module_0.TrainerConfig.enable_progress_bar is True
    assert module_0.TrainerConfig.overfit_batches == pytest.approx(0.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.track_grad_norm == -1
    assert module_0.TrainerConfig.check_val_every_n_epoch == 1
    assert module_0.TrainerConfig.fast_dev_run is False
    assert module_0.TrainerConfig.accumulate_grad_batches == 1
    assert module_0.TrainerConfig.max_epochs == 1000
    assert module_0.TrainerConfig.min_epochs == 1
    assert module_0.TrainerConfig.max_steps == -1
    assert module_0.TrainerConfig.min_steps is None
    assert module_0.TrainerConfig.limit_train_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.limit_val_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.limit_test_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.val_check_interval == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.log_every_n_steps == 50
    assert module_0.TrainerConfig.accelerator is None
    assert module_0.TrainerConfig.sync_batchnorm is False
    assert module_0.TrainerConfig.precision == 32
    assert module_0.TrainerConfig.num_sanity_val_steps == 2
    assert module_0.TrainerConfig.resume_from_checkpoint is None
    assert module_0.TrainerConfig.profiler is None
    assert module_0.TrainerConfig.benchmark is False
    assert module_0.TrainerConfig.deterministic is False
    assert module_0.TrainerConfig.auto_lr_find is False
    assert module_0.TrainerConfig.replace_sampler_ddp is True
    assert module_0.TrainerConfig.detect_anomaly is False
    assert module_0.TrainerConfig.auto_scale_batch_size is False
    assert module_0.TrainerConfig.amp_backend == "native"
    assert module_0.TrainerConfig.amp_level is None
    assert module_0.TrainerConfig.plugins is None
    assert module_0.TrainerConfig.move_metrics_to_cpu is False
    assert module_0.TrainerConfig.multiple_trainloader_mode == "max_size_cycle"
    assert module_0.TrainerConfig.limit_predict_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.gradient_clip_algorithm == "norm"
    assert module_0.TrainerConfig.max_time is None
    assert module_0.TrainerConfig.reload_dataloaders_every_n_epochs == 0
    assert module_0.TrainerConfig.ipus is None
    assert module_0.TrainerConfig.devices is None
    assert module_0.TrainerConfig.strategy is None
    assert module_0.TrainerConfig.enable_checkpointing is False
    assert module_0.TrainerConfig.enable_model_summary is True


def test_case_4():
    str_0 = None
    int_0 = -130
    bool_0 = False
    float_0 = 0.1
    trainer_config_0 = module_0.TrainerConfig(
        str_0, int_0, int_0, str_0, str_0, bool_0, bool_0, int_0, bool_0, str_0, float_0, str_0, int_0, bool_0
    )
    assert trainer_config_0.logger is None
    assert trainer_config_0.callbacks == -130
    assert trainer_config_0.default_root_dir == -130
    assert trainer_config_0.gradient_clip_val is None
    assert trainer_config_0.num_nodes is None
    assert trainer_config_0.gpus is False
    assert trainer_config_0.auto_select_gpus is False
    assert trainer_config_0.tpu_cores == -130
    assert trainer_config_0.enable_progress_bar is False
    assert trainer_config_0.overfit_batches is None
    assert trainer_config_0.track_grad_norm == pytest.approx(0.1, abs=0.01, rel=0.01)
    assert trainer_config_0.check_val_every_n_epoch is None
    assert trainer_config_0.fast_dev_run == -130
    assert trainer_config_0.accumulate_grad_batches is False
    assert trainer_config_0.max_epochs == 1000
    assert trainer_config_0.min_epochs == 1
    assert trainer_config_0.max_steps == -1
    assert trainer_config_0.min_steps is None
    assert trainer_config_0.limit_train_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_0.limit_val_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_0.limit_test_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_0.val_check_interval == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_0.log_every_n_steps == 50
    assert trainer_config_0.accelerator is None
    assert trainer_config_0.sync_batchnorm is False
    assert trainer_config_0.precision == 32
    assert trainer_config_0.num_sanity_val_steps == 2
    assert trainer_config_0.resume_from_checkpoint is None
    assert trainer_config_0.profiler is None
    assert trainer_config_0.benchmark is False
    assert trainer_config_0.deterministic is False
    assert trainer_config_0.auto_lr_find is False
    assert trainer_config_0.replace_sampler_ddp is True
    assert trainer_config_0.detect_anomaly is False
    assert trainer_config_0.auto_scale_batch_size is False
    assert trainer_config_0.amp_backend == "native"
    assert trainer_config_0.amp_level is None
    assert trainer_config_0.plugins is None
    assert trainer_config_0.move_metrics_to_cpu is False
    assert trainer_config_0.multiple_trainloader_mode == "max_size_cycle"
    assert trainer_config_0.limit_predict_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_0.gradient_clip_algorithm == "norm"
    assert trainer_config_0.max_time is None
    assert trainer_config_0.reload_dataloaders_every_n_epochs == 0
    assert trainer_config_0.ipus is None
    assert trainer_config_0.devices is None
    assert trainer_config_0.strategy is None
    assert trainer_config_0.enable_checkpointing is False
    assert trainer_config_0.enable_model_summary is True
    assert module_0.TrainerConfig.logger is True
    assert module_0.TrainerConfig.callbacks is None
    assert module_0.TrainerConfig.default_root_dir is None
    assert module_0.TrainerConfig.gradient_clip_val == 0
    assert module_0.TrainerConfig.num_nodes == 1
    assert module_0.TrainerConfig.gpus is None
    assert module_0.TrainerConfig.auto_select_gpus is False
    assert module_0.TrainerConfig.tpu_cores is None
    assert module_0.TrainerConfig.enable_progress_bar is True
    assert module_0.TrainerConfig.overfit_batches == pytest.approx(0.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.track_grad_norm == -1
    assert module_0.TrainerConfig.check_val_every_n_epoch == 1
    assert module_0.TrainerConfig.fast_dev_run is False
    assert module_0.TrainerConfig.accumulate_grad_batches == 1
    assert module_0.TrainerConfig.max_epochs == 1000
    assert module_0.TrainerConfig.min_epochs == 1
    assert module_0.TrainerConfig.max_steps == -1
    assert module_0.TrainerConfig.min_steps is None
    assert module_0.TrainerConfig.limit_train_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.limit_val_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.limit_test_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.val_check_interval == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.log_every_n_steps == 50
    assert module_0.TrainerConfig.accelerator is None
    assert module_0.TrainerConfig.sync_batchnorm is False
    assert module_0.TrainerConfig.precision == 32
    assert module_0.TrainerConfig.num_sanity_val_steps == 2
    assert module_0.TrainerConfig.resume_from_checkpoint is None
    assert module_0.TrainerConfig.profiler is None
    assert module_0.TrainerConfig.benchmark is False
    assert module_0.TrainerConfig.deterministic is False
    assert module_0.TrainerConfig.auto_lr_find is False
    assert module_0.TrainerConfig.replace_sampler_ddp is True
    assert module_0.TrainerConfig.detect_anomaly is False
    assert module_0.TrainerConfig.auto_scale_batch_size is False
    assert module_0.TrainerConfig.amp_backend == "native"
    assert module_0.TrainerConfig.amp_level is None
    assert module_0.TrainerConfig.plugins is None
    assert module_0.TrainerConfig.move_metrics_to_cpu is False
    assert module_0.TrainerConfig.multiple_trainloader_mode == "max_size_cycle"
    assert module_0.TrainerConfig.limit_predict_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.gradient_clip_algorithm == "norm"
    assert module_0.TrainerConfig.max_time is None
    assert module_0.TrainerConfig.reload_dataloaders_every_n_epochs == 0
    assert module_0.TrainerConfig.ipus is None
    assert module_0.TrainerConfig.devices is None
    assert module_0.TrainerConfig.strategy is None
    assert module_0.TrainerConfig.enable_checkpointing is False
    assert module_0.TrainerConfig.enable_model_summary is True


def test_case_5():
    list_0 = []
    any_0 = None
    str_0 = "e"
    float_0 = None
    int_0 = None
    int_1 = 103
    int_2 = None
    bool_0 = False
    bool_1 = None
    float_1 = 1898.11652
    trainer_config_0 = module_0.TrainerConfig(
        any_0,
        str_0,
        float_0,
        int_0,
        any_0,
        str_0,
        int_0,
        int_0,
        int_0,
        float_0,
        int_1,
        int_2,
        bool_0,
        int_2,
        str_0,
        any_0,
        bool_1,
        float_1,
    )
    assert trainer_config_0.logger is None
    assert trainer_config_0.callbacks == "e"
    assert trainer_config_0.default_root_dir is None
    assert trainer_config_0.gradient_clip_val is None
    assert trainer_config_0.num_nodes is None
    assert trainer_config_0.gpus == "e"
    assert trainer_config_0.auto_select_gpus is None
    assert trainer_config_0.tpu_cores is None
    assert trainer_config_0.enable_progress_bar is None
    assert trainer_config_0.overfit_batches is None
    assert trainer_config_0.track_grad_norm == 103
    assert trainer_config_0.check_val_every_n_epoch is None
    assert trainer_config_0.fast_dev_run is False
    assert trainer_config_0.accumulate_grad_batches is None
    assert trainer_config_0.max_epochs == "e"
    assert trainer_config_0.min_epochs is None
    assert trainer_config_0.max_steps is None
    assert trainer_config_0.min_steps == pytest.approx(1898.11652, abs=0.01, rel=0.01)
    assert trainer_config_0.limit_train_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_0.limit_val_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_0.limit_test_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_0.val_check_interval == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_0.log_every_n_steps == 50
    assert trainer_config_0.accelerator is None
    assert trainer_config_0.sync_batchnorm is False
    assert trainer_config_0.precision == 32
    assert trainer_config_0.num_sanity_val_steps == 2
    assert trainer_config_0.resume_from_checkpoint is None
    assert trainer_config_0.profiler is None
    assert trainer_config_0.benchmark is False
    assert trainer_config_0.deterministic is False
    assert trainer_config_0.auto_lr_find is False
    assert trainer_config_0.replace_sampler_ddp is True
    assert trainer_config_0.detect_anomaly is False
    assert trainer_config_0.auto_scale_batch_size is False
    assert trainer_config_0.amp_backend == "native"
    assert trainer_config_0.amp_level is None
    assert trainer_config_0.plugins is None
    assert trainer_config_0.move_metrics_to_cpu is False
    assert trainer_config_0.multiple_trainloader_mode == "max_size_cycle"
    assert trainer_config_0.limit_predict_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_0.gradient_clip_algorithm == "norm"
    assert trainer_config_0.max_time is None
    assert trainer_config_0.reload_dataloaders_every_n_epochs == 0
    assert trainer_config_0.ipus is None
    assert trainer_config_0.devices is None
    assert trainer_config_0.strategy is None
    assert trainer_config_0.enable_checkpointing is False
    assert trainer_config_0.enable_model_summary is True
    assert module_0.TrainerConfig.logger is True
    assert module_0.TrainerConfig.callbacks is None
    assert module_0.TrainerConfig.default_root_dir is None
    assert module_0.TrainerConfig.gradient_clip_val == 0
    assert module_0.TrainerConfig.num_nodes == 1
    assert module_0.TrainerConfig.gpus is None
    assert module_0.TrainerConfig.auto_select_gpus is False
    assert module_0.TrainerConfig.tpu_cores is None
    assert module_0.TrainerConfig.enable_progress_bar is True
    assert module_0.TrainerConfig.overfit_batches == pytest.approx(0.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.track_grad_norm == -1
    assert module_0.TrainerConfig.check_val_every_n_epoch == 1
    assert module_0.TrainerConfig.fast_dev_run is False
    assert module_0.TrainerConfig.accumulate_grad_batches == 1
    assert module_0.TrainerConfig.max_epochs == 1000
    assert module_0.TrainerConfig.min_epochs == 1
    assert module_0.TrainerConfig.max_steps == -1
    assert module_0.TrainerConfig.min_steps is None
    assert module_0.TrainerConfig.limit_train_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.limit_val_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.limit_test_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.val_check_interval == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.log_every_n_steps == 50
    assert module_0.TrainerConfig.accelerator is None
    assert module_0.TrainerConfig.sync_batchnorm is False
    assert module_0.TrainerConfig.precision == 32
    assert module_0.TrainerConfig.num_sanity_val_steps == 2
    assert module_0.TrainerConfig.resume_from_checkpoint is None
    assert module_0.TrainerConfig.profiler is None
    assert module_0.TrainerConfig.benchmark is False
    assert module_0.TrainerConfig.deterministic is False
    assert module_0.TrainerConfig.auto_lr_find is False
    assert module_0.TrainerConfig.replace_sampler_ddp is True
    assert module_0.TrainerConfig.detect_anomaly is False
    assert module_0.TrainerConfig.auto_scale_batch_size is False
    assert module_0.TrainerConfig.amp_backend == "native"
    assert module_0.TrainerConfig.amp_level is None
    assert module_0.TrainerConfig.plugins is None
    assert module_0.TrainerConfig.move_metrics_to_cpu is False
    assert module_0.TrainerConfig.multiple_trainloader_mode == "max_size_cycle"
    assert module_0.TrainerConfig.limit_predict_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.gradient_clip_algorithm == "norm"
    assert module_0.TrainerConfig.max_time is None
    assert module_0.TrainerConfig.reload_dataloaders_every_n_epochs == 0
    assert module_0.TrainerConfig.ipus is None
    assert module_0.TrainerConfig.devices is None
    assert module_0.TrainerConfig.strategy is None
    assert module_0.TrainerConfig.enable_checkpointing is False
    assert module_0.TrainerConfig.enable_model_summary is True
    str_1 = "r\x0c7\n"
    int_3 = -1855
    int_4 = 933
    bool_2 = False
    str_2 = "FhqU2\tE?%9&\t1w"
    bool_3 = False
    trainer_config_1 = module_0.TrainerConfig(
        bool_1,
        trainer_config_0,
        int_3,
        int_4,
        float_0,
        int_2,
        int_3,
        bool_0,
        any_0,
        bool_0,
        bool_2,
        str_0,
        str_2,
        str_2,
        any_0,
        bool_3,
        float_1,
        any_0,
    )
    assert trainer_config_1.logger is None
    assert trainer_config_1.default_root_dir == -1855
    assert trainer_config_1.gradient_clip_val == 933
    assert trainer_config_1.num_nodes is None
    assert trainer_config_1.gpus is None
    assert trainer_config_1.auto_select_gpus == -1855
    assert trainer_config_1.tpu_cores is False
    assert trainer_config_1.enable_progress_bar is None
    assert trainer_config_1.overfit_batches is False
    assert trainer_config_1.track_grad_norm is False
    assert trainer_config_1.check_val_every_n_epoch == "e"
    assert trainer_config_1.fast_dev_run == "FhqU2\tE?%9&\t1w"
    assert trainer_config_1.accumulate_grad_batches == "FhqU2\tE?%9&\t1w"
    assert trainer_config_1.max_epochs is None
    assert trainer_config_1.min_epochs is False
    assert trainer_config_1.max_steps == pytest.approx(1898.11652, abs=0.01, rel=0.01)
    assert trainer_config_1.min_steps is None
    assert trainer_config_1.limit_train_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_1.limit_val_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_1.limit_test_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_1.val_check_interval == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_1.log_every_n_steps == 50
    assert trainer_config_1.accelerator is None
    assert trainer_config_1.sync_batchnorm is False
    assert trainer_config_1.precision == 32
    assert trainer_config_1.num_sanity_val_steps == 2
    assert trainer_config_1.resume_from_checkpoint is None
    assert trainer_config_1.profiler is None
    assert trainer_config_1.benchmark is False
    assert trainer_config_1.deterministic is False
    assert trainer_config_1.auto_lr_find is False
    assert trainer_config_1.replace_sampler_ddp is True
    assert trainer_config_1.detect_anomaly is False
    assert trainer_config_1.auto_scale_batch_size is False
    assert trainer_config_1.amp_backend == "native"
    assert trainer_config_1.amp_level is None
    assert trainer_config_1.plugins is None
    assert trainer_config_1.move_metrics_to_cpu is False
    assert trainer_config_1.multiple_trainloader_mode == "max_size_cycle"
    assert trainer_config_1.limit_predict_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_1.gradient_clip_algorithm == "norm"
    assert trainer_config_1.max_time is None
    assert trainer_config_1.reload_dataloaders_every_n_epochs == 0
    assert trainer_config_1.ipus is None
    assert trainer_config_1.devices is None
    assert trainer_config_1.strategy is None
    assert trainer_config_1.enable_checkpointing is False
    assert trainer_config_1.enable_model_summary is True
    float_2 = -158.8
    int_5 = 1191
    bool_4 = False
    any_1 = None
    bool_5 = True
    bool_6 = False
    bool_7 = True
    bool_8 = False
    str_3 = "SUCCESS"
    int_6 = 95
    trainer_config_2 = module_0.TrainerConfig(
        any_1, bool_7, int_1, int_5, int_3, str_0, bool_8, str_1, bool_4, str_3, str_1, any_0, int_6, int_2, bool_6
    )
    assert trainer_config_2.logger is None
    assert trainer_config_2.callbacks is True
    assert trainer_config_2.default_root_dir == 103
    assert trainer_config_2.gradient_clip_val == 1191
    assert trainer_config_2.num_nodes == -1855
    assert trainer_config_2.gpus == "e"
    assert trainer_config_2.auto_select_gpus is False
    assert trainer_config_2.tpu_cores == "r\x0c7\n"
    assert trainer_config_2.enable_progress_bar is False
    assert trainer_config_2.overfit_batches == "SUCCESS"
    assert trainer_config_2.track_grad_norm == "r\x0c7\n"
    assert trainer_config_2.check_val_every_n_epoch is None
    assert trainer_config_2.fast_dev_run == 95
    assert trainer_config_2.accumulate_grad_batches is None
    assert trainer_config_2.max_epochs is False
    assert trainer_config_2.min_epochs == 1
    assert trainer_config_2.max_steps == -1
    assert trainer_config_2.min_steps is None
    assert trainer_config_2.limit_train_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_2.limit_val_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_2.limit_test_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_2.val_check_interval == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_2.log_every_n_steps == 50
    assert trainer_config_2.accelerator is None
    assert trainer_config_2.sync_batchnorm is False
    assert trainer_config_2.precision == 32
    assert trainer_config_2.num_sanity_val_steps == 2
    assert trainer_config_2.resume_from_checkpoint is None
    assert trainer_config_2.profiler is None
    assert trainer_config_2.benchmark is False
    assert trainer_config_2.deterministic is False
    assert trainer_config_2.auto_lr_find is False
    assert trainer_config_2.replace_sampler_ddp is True
    assert trainer_config_2.detect_anomaly is False
    assert trainer_config_2.auto_scale_batch_size is False
    assert trainer_config_2.amp_backend == "native"
    assert trainer_config_2.amp_level is None
    assert trainer_config_2.plugins is None
    assert trainer_config_2.move_metrics_to_cpu is False
    assert trainer_config_2.multiple_trainloader_mode == "max_size_cycle"
    assert trainer_config_2.limit_predict_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_2.gradient_clip_algorithm == "norm"
    assert trainer_config_2.max_time is None
    assert trainer_config_2.reload_dataloaders_every_n_epochs == 0
    assert trainer_config_2.ipus is None
    assert trainer_config_2.devices is None
    assert trainer_config_2.strategy is None
    assert trainer_config_2.enable_checkpointing is False
    assert trainer_config_2.enable_model_summary is True
    str_4 = "@N_<*,l`nfy8O/b"
    bool_9 = False
    bool_10 = False
    trainer_config_3 = module_0.TrainerConfig(
        list_0,
        str_1,
        float_2,
        int_5,
        bool_4,
        bool_4,
        int_5,
        any_1,
        bool_5,
        bool_6,
        str_1,
        str_4,
        int_5,
        int_5,
        bool_9,
        bool_10,
    )
    assert trainer_config_3.logger == []
    assert trainer_config_3.callbacks == "r\x0c7\n"
    assert trainer_config_3.default_root_dir == pytest.approx(-158.8, abs=0.01, rel=0.01)
    assert trainer_config_3.gradient_clip_val == 1191
    assert trainer_config_3.num_nodes is False
    assert trainer_config_3.gpus is False
    assert trainer_config_3.auto_select_gpus == 1191
    assert trainer_config_3.tpu_cores is None
    assert trainer_config_3.enable_progress_bar is True
    assert trainer_config_3.overfit_batches is False
    assert trainer_config_3.track_grad_norm == "r\x0c7\n"
    assert trainer_config_3.check_val_every_n_epoch == "@N_<*,l`nfy8O/b"
    assert trainer_config_3.fast_dev_run == 1191
    assert trainer_config_3.accumulate_grad_batches == 1191
    assert trainer_config_3.max_epochs is False
    assert trainer_config_3.min_epochs is False
    assert trainer_config_3.max_steps == -1
    assert trainer_config_3.min_steps is None
    assert trainer_config_3.limit_train_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_3.limit_val_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_3.limit_test_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_3.val_check_interval == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_3.log_every_n_steps == 50
    assert trainer_config_3.accelerator is None
    assert trainer_config_3.sync_batchnorm is False
    assert trainer_config_3.precision == 32
    assert trainer_config_3.num_sanity_val_steps == 2
    assert trainer_config_3.resume_from_checkpoint is None
    assert trainer_config_3.profiler is None
    assert trainer_config_3.benchmark is False
    assert trainer_config_3.deterministic is False
    assert trainer_config_3.auto_lr_find is False
    assert trainer_config_3.replace_sampler_ddp is True
    assert trainer_config_3.detect_anomaly is False
    assert trainer_config_3.auto_scale_batch_size is False
    assert trainer_config_3.amp_backend == "native"
    assert trainer_config_3.amp_level is None
    assert trainer_config_3.plugins is None
    assert trainer_config_3.move_metrics_to_cpu is False
    assert trainer_config_3.multiple_trainloader_mode == "max_size_cycle"
    assert trainer_config_3.limit_predict_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_3.gradient_clip_algorithm == "norm"
    assert trainer_config_3.max_time is None
    assert trainer_config_3.reload_dataloaders_every_n_epochs == 0
    assert trainer_config_3.ipus is None
    assert trainer_config_3.devices is None
    assert trainer_config_3.strategy is None
    assert trainer_config_3.enable_checkpointing is False
    assert trainer_config_3.enable_model_summary is True


def test_case_6():
    str_0 = "\\wbSW&4F`\x0b\r~cs5j8"
    float_0 = -1394.79
    bool_0 = False
    int_0 = 107
    int_1 = 119
    bool_1 = True
    int_2 = -2698
    int_3 = 13
    var_0 = None
    set_0 = None
    str_1 = "o2d\\G'Xs3a/c\x0c"
    bool_2 = False
    str_2 = "n,bHT\n=%|Pae"
    bool_3 = False
    int_4 = 16
    bool_4 = False
    dict_0 = {str_2: int_0, bool_1: int_2}
    bool_5 = None
    bool_6 = True
    bool_7 = False
    int_5 = 2213
    trainer_config_0 = module_0.TrainerConfig(
        bool_0,
        str_0,
        str_1,
        bool_2,
        str_2,
        set_0,
        bool_3,
        bool_1,
        int_4,
        bool_4,
        str_2,
        bool_4,
        str_1,
        dict_0,
        bool_5,
        bool_6,
        bool_7,
        float_0,
        int_5,
        bool_0,
    )
    assert trainer_config_0.logger is False
    assert trainer_config_0.callbacks == "\\wbSW&4F`\x0b\r~cs5j8"
    assert trainer_config_0.default_root_dir == "o2d\\G'Xs3a/c\x0c"
    assert trainer_config_0.gradient_clip_val is False
    assert trainer_config_0.num_nodes == "n,bHT\n=%|Pae"
    assert trainer_config_0.gpus is None
    assert trainer_config_0.auto_select_gpus is False
    assert trainer_config_0.tpu_cores is True
    assert trainer_config_0.enable_progress_bar == 16
    assert trainer_config_0.overfit_batches is False
    assert trainer_config_0.track_grad_norm == "n,bHT\n=%|Pae"
    assert trainer_config_0.check_val_every_n_epoch is False
    assert trainer_config_0.fast_dev_run == "o2d\\G'Xs3a/c\x0c"
    assert trainer_config_0.accumulate_grad_batches == {"n,bHT\n=%|Pae": 107, (True): -2698}
    assert trainer_config_0.max_epochs is None
    assert trainer_config_0.min_epochs is True
    assert trainer_config_0.max_steps is False
    assert trainer_config_0.min_steps == pytest.approx(-1394.79, abs=0.01, rel=0.01)
    assert trainer_config_0.limit_train_batches == 2213
    assert trainer_config_0.limit_val_batches is False
    assert trainer_config_0.limit_test_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_0.val_check_interval == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_0.log_every_n_steps == 50
    assert trainer_config_0.accelerator is None
    assert trainer_config_0.sync_batchnorm is False
    assert trainer_config_0.precision == 32
    assert trainer_config_0.num_sanity_val_steps == 2
    assert trainer_config_0.resume_from_checkpoint is None
    assert trainer_config_0.profiler is None
    assert trainer_config_0.benchmark is False
    assert trainer_config_0.deterministic is False
    assert trainer_config_0.auto_lr_find is False
    assert trainer_config_0.replace_sampler_ddp is True
    assert trainer_config_0.detect_anomaly is False
    assert trainer_config_0.auto_scale_batch_size is False
    assert trainer_config_0.amp_backend == "native"
    assert trainer_config_0.amp_level is None
    assert trainer_config_0.plugins is None
    assert trainer_config_0.move_metrics_to_cpu is False
    assert trainer_config_0.multiple_trainloader_mode == "max_size_cycle"
    assert trainer_config_0.limit_predict_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_0.gradient_clip_algorithm == "norm"
    assert trainer_config_0.max_time is None
    assert trainer_config_0.reload_dataloaders_every_n_epochs == 0
    assert trainer_config_0.ipus is None
    assert trainer_config_0.devices is None
    assert trainer_config_0.strategy is None
    assert trainer_config_0.enable_checkpointing is False
    assert trainer_config_0.enable_model_summary is True
    assert module_0.TrainerConfig.logger is True
    assert module_0.TrainerConfig.callbacks is None
    assert module_0.TrainerConfig.default_root_dir is None
    assert module_0.TrainerConfig.gradient_clip_val == 0
    assert module_0.TrainerConfig.num_nodes == 1
    assert module_0.TrainerConfig.gpus is None
    assert module_0.TrainerConfig.auto_select_gpus is False
    assert module_0.TrainerConfig.tpu_cores is None
    assert module_0.TrainerConfig.enable_progress_bar is True
    assert module_0.TrainerConfig.overfit_batches == pytest.approx(0.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.track_grad_norm == -1
    assert module_0.TrainerConfig.check_val_every_n_epoch == 1
    assert module_0.TrainerConfig.fast_dev_run is False
    assert module_0.TrainerConfig.accumulate_grad_batches == 1
    assert module_0.TrainerConfig.max_epochs == 1000
    assert module_0.TrainerConfig.min_epochs == 1
    assert module_0.TrainerConfig.max_steps == -1
    assert module_0.TrainerConfig.min_steps is None
    assert module_0.TrainerConfig.limit_train_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.limit_val_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.limit_test_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.val_check_interval == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.log_every_n_steps == 50
    assert module_0.TrainerConfig.accelerator is None
    assert module_0.TrainerConfig.sync_batchnorm is False
    assert module_0.TrainerConfig.precision == 32
    assert module_0.TrainerConfig.num_sanity_val_steps == 2
    assert module_0.TrainerConfig.resume_from_checkpoint is None
    assert module_0.TrainerConfig.profiler is None
    assert module_0.TrainerConfig.benchmark is False
    assert module_0.TrainerConfig.deterministic is False
    assert module_0.TrainerConfig.auto_lr_find is False
    assert module_0.TrainerConfig.replace_sampler_ddp is True
    assert module_0.TrainerConfig.detect_anomaly is False
    assert module_0.TrainerConfig.auto_scale_batch_size is False
    assert module_0.TrainerConfig.amp_backend == "native"
    assert module_0.TrainerConfig.amp_level is None
    assert module_0.TrainerConfig.plugins is None
    assert module_0.TrainerConfig.move_metrics_to_cpu is False
    assert module_0.TrainerConfig.multiple_trainloader_mode == "max_size_cycle"
    assert module_0.TrainerConfig.limit_predict_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert module_0.TrainerConfig.gradient_clip_algorithm == "norm"
    assert module_0.TrainerConfig.max_time is None
    assert module_0.TrainerConfig.reload_dataloaders_every_n_epochs == 0
    assert module_0.TrainerConfig.ipus is None
    assert module_0.TrainerConfig.devices is None
    assert module_0.TrainerConfig.strategy is None
    assert module_0.TrainerConfig.enable_checkpointing is False
    assert module_0.TrainerConfig.enable_model_summary is True
    bool_8 = True
    str_3 = "mOA=\tB"
    tuple_0 = set_0, set_0, bool_8, str_3
    trainer_config_1 = module_0.TrainerConfig(
        str_0, int_1, bool_1, int_2, bool_0, float_0, int_3, int_1, str_0, str_0, bool_1, var_0, str_0, tuple_0, bool_8
    )
    assert trainer_config_1.logger == "\\wbSW&4F`\x0b\r~cs5j8"
    assert trainer_config_1.callbacks == 119
    assert trainer_config_1.default_root_dir is True
    assert trainer_config_1.gradient_clip_val == -2698
    assert trainer_config_1.num_nodes is False
    assert trainer_config_1.gpus == pytest.approx(-1394.79, abs=0.01, rel=0.01)
    assert trainer_config_1.auto_select_gpus == 13
    assert trainer_config_1.tpu_cores == 119
    assert trainer_config_1.enable_progress_bar == "\\wbSW&4F`\x0b\r~cs5j8"
    assert trainer_config_1.overfit_batches == "\\wbSW&4F`\x0b\r~cs5j8"
    assert trainer_config_1.track_grad_norm is True
    assert trainer_config_1.check_val_every_n_epoch is None
    assert trainer_config_1.fast_dev_run == "\\wbSW&4F`\x0b\r~cs5j8"
    assert trainer_config_1.accumulate_grad_batches == (None, None, True, "mOA=\tB")
    assert trainer_config_1.max_epochs is True
    assert trainer_config_1.min_epochs == 1
    assert trainer_config_1.max_steps == -1
    assert trainer_config_1.min_steps is None
    assert trainer_config_1.limit_train_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_1.limit_val_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_1.limit_test_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_1.val_check_interval == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_1.log_every_n_steps == 50
    assert trainer_config_1.accelerator is None
    assert trainer_config_1.sync_batchnorm is False
    assert trainer_config_1.precision == 32
    assert trainer_config_1.num_sanity_val_steps == 2
    assert trainer_config_1.resume_from_checkpoint is None
    assert trainer_config_1.profiler is None
    assert trainer_config_1.benchmark is False
    assert trainer_config_1.deterministic is False
    assert trainer_config_1.auto_lr_find is False
    assert trainer_config_1.replace_sampler_ddp is True
    assert trainer_config_1.detect_anomaly is False
    assert trainer_config_1.auto_scale_batch_size is False
    assert trainer_config_1.amp_backend == "native"
    assert trainer_config_1.amp_level is None
    assert trainer_config_1.plugins is None
    assert trainer_config_1.move_metrics_to_cpu is False
    assert trainer_config_1.multiple_trainloader_mode == "max_size_cycle"
    assert trainer_config_1.limit_predict_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_1.gradient_clip_algorithm == "norm"
    assert trainer_config_1.max_time is None
    assert trainer_config_1.reload_dataloaders_every_n_epochs == 0
    assert trainer_config_1.ipus is None
    assert trainer_config_1.devices is None
    assert trainer_config_1.strategy is None
    assert trainer_config_1.enable_checkpointing is False
    assert trainer_config_1.enable_model_summary is True
    set_1 = {float_0, float_0}
    int_6 = 49
    bool_9 = True
    float_1 = 1937.77703
    str_4 = """This class represents axis semantics and (optionally) it's dimensionality

    Parameters
    ----------
    kind: what kind of axis it is? For example Batch, Height, etc.
        AxisKindAbstract
    size: specify if the axis should have a fixed size. By default, it is set to None and you typically do not want to
    set it for Batch and Time.
        (int, optional)
    is_list: whether this is a list or a tensor axis.
        (bool, default=False)
    """
    bytes_0 = b"\xc7\xf6\xcd\xff-\xa9\xd1&\x16\xaa\xd6,\x97\x95\xf7\xb8e"
    bool_10 = False
    trainer_config_2 = module_0.TrainerConfig(
        float_0,
        bool_0,
        int_0,
        set_1,
        int_0,
        int_6,
        int_6,
        bool_0,
        str_0,
        bool_9,
        float_1,
        str_4,
        int_6,
        bytes_0,
        set_1,
        bool_10,
        bool_0,
    )
    assert trainer_config_2.logger == pytest.approx(-1394.79, abs=0.01, rel=0.01)
    assert trainer_config_2.callbacks is False
    assert trainer_config_2.default_root_dir == 107
    assert len(trainer_config_2.gradient_clip_val) == 1
    assert trainer_config_2.num_nodes == 107
    assert trainer_config_2.gpus == 49
    assert trainer_config_2.auto_select_gpus == 49
    assert trainer_config_2.tpu_cores is False
    assert trainer_config_2.enable_progress_bar == "\\wbSW&4F`\x0b\r~cs5j8"
    assert trainer_config_2.overfit_batches is True
    assert trainer_config_2.track_grad_norm == pytest.approx(1937.77703, abs=0.01, rel=0.01)
    assert (
        trainer_config_2.check_val_every_n_epoch
        == """This class represents axis semantics and (optionally) it's dimensionality

    Parameters
    ----------
    kind: what kind of axis it is? For example Batch, Height, etc.
        AxisKindAbstract
    size: specify if the axis should have a fixed size. By default, it is set to None and you typically do not want to
    set it for Batch and Time.
        (int, optional)
    is_list: whether this is a list or a tensor axis.
        (bool, default=False)
    """
    )
    assert trainer_config_2.fast_dev_run == 49
    assert trainer_config_2.accumulate_grad_batches == b"\xc7\xf6\xcd\xff-\xa9\xd1&\x16\xaa\xd6,\x97\x95\xf7\xb8e"
    assert len(trainer_config_2.max_epochs) == 1
    assert trainer_config_2.min_epochs is False
    assert trainer_config_2.max_steps is False
    assert trainer_config_2.min_steps is None
    assert trainer_config_2.limit_train_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_2.limit_val_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_2.limit_test_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_2.val_check_interval == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_2.log_every_n_steps == 50
    assert trainer_config_2.accelerator is None
    assert trainer_config_2.sync_batchnorm is False
    assert trainer_config_2.precision == 32
    assert trainer_config_2.num_sanity_val_steps == 2
    assert trainer_config_2.resume_from_checkpoint is None
    assert trainer_config_2.profiler is None
    assert trainer_config_2.benchmark is False
    assert trainer_config_2.deterministic is False
    assert trainer_config_2.auto_lr_find is False
    assert trainer_config_2.replace_sampler_ddp is True
    assert trainer_config_2.detect_anomaly is False
    assert trainer_config_2.auto_scale_batch_size is False
    assert trainer_config_2.amp_backend == "native"
    assert trainer_config_2.amp_level is None
    assert trainer_config_2.plugins is None
    assert trainer_config_2.move_metrics_to_cpu is False
    assert trainer_config_2.multiple_trainloader_mode == "max_size_cycle"
    assert trainer_config_2.limit_predict_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_2.gradient_clip_algorithm == "norm"
    assert trainer_config_2.max_time is None
    assert trainer_config_2.reload_dataloaders_every_n_epochs == 0
    assert trainer_config_2.ipus is None
    assert trainer_config_2.devices is None
    assert trainer_config_2.strategy is None
    assert trainer_config_2.enable_checkpointing is False
    assert trainer_config_2.enable_model_summary is True
    bool_11 = True
    bool_12 = True
    int_7 = 3101
    bool_13 = True
    list_0 = []
    bool_14 = True
    bool_15 = True
    str_5 = "f&~[{pe"
    bool_16 = True
    str_6 = "LPDNET"
    trainer_config_3 = module_0.TrainerConfig(
        str_0,
        bool_11,
        bool_12,
        bool_11,
        int_7,
        str_0,
        bool_13,
        bool_13,
        list_0,
        bool_14,
        bool_13,
        bool_15,
        str_5,
        str_5,
        bool_16,
        str_6,
        int_7,
        list_0,
    )
    assert trainer_config_3.logger == "\\wbSW&4F`\x0b\r~cs5j8"
    assert trainer_config_3.callbacks is True
    assert trainer_config_3.default_root_dir is True
    assert trainer_config_3.gradient_clip_val is True
    assert trainer_config_3.num_nodes == 3101
    assert trainer_config_3.gpus == "\\wbSW&4F`\x0b\r~cs5j8"
    assert trainer_config_3.auto_select_gpus is True
    assert trainer_config_3.tpu_cores is True
    assert trainer_config_3.enable_progress_bar == []
    assert trainer_config_3.overfit_batches is True
    assert trainer_config_3.track_grad_norm is True
    assert trainer_config_3.check_val_every_n_epoch is True
    assert trainer_config_3.fast_dev_run == "f&~[{pe"
    assert trainer_config_3.accumulate_grad_batches == "f&~[{pe"
    assert trainer_config_3.max_epochs is True
    assert trainer_config_3.min_epochs == "LPDNET"
    assert trainer_config_3.max_steps == 3101
    assert trainer_config_3.min_steps == []
    assert trainer_config_3.limit_train_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_3.limit_val_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_3.limit_test_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_3.val_check_interval == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_3.log_every_n_steps == 50
    assert trainer_config_3.accelerator is None
    assert trainer_config_3.sync_batchnorm is False
    assert trainer_config_3.precision == 32
    assert trainer_config_3.num_sanity_val_steps == 2
    assert trainer_config_3.resume_from_checkpoint is None
    assert trainer_config_3.profiler is None
    assert trainer_config_3.benchmark is False
    assert trainer_config_3.deterministic is False
    assert trainer_config_3.auto_lr_find is False
    assert trainer_config_3.replace_sampler_ddp is True
    assert trainer_config_3.detect_anomaly is False
    assert trainer_config_3.auto_scale_batch_size is False
    assert trainer_config_3.amp_backend == "native"
    assert trainer_config_3.amp_level is None
    assert trainer_config_3.plugins is None
    assert trainer_config_3.move_metrics_to_cpu is False
    assert trainer_config_3.multiple_trainloader_mode == "max_size_cycle"
    assert trainer_config_3.limit_predict_batches == pytest.approx(1.0, abs=0.01, rel=0.01)
    assert trainer_config_3.gradient_clip_algorithm == "norm"
    assert trainer_config_3.max_time is None
    assert trainer_config_3.reload_dataloaders_every_n_epochs == 0
    assert trainer_config_3.ipus is None
    assert trainer_config_3.devices is None
    assert trainer_config_3.strategy is None
    assert trainer_config_3.enable_checkpointing is False
    assert trainer_config_3.enable_model_summary is True
